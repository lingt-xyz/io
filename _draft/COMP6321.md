---
title: COMP 6321
---

Goals of the course:


1. Understand how ML algorithms work
    - Theory: motivations, math, concepts
    - Practice: code from scratch, use libraries
    - Research: propose your own ideas
2. Become a solid ML practitioner
    - Preprocess data, train models, evaluate
3. Ace an ML job interview
    - Fluent in reading/writing/talking about ML


## Q & A

### What is ML about

Machine learning (ML) is 
- The scientific study of algorithms and statistical models that computer systems use to perform a specific task _without using explicit instructions_, relying on patterns and inference instead. 
- It is seen as a _subset_ of artificial intelligence. 
- Machine learning algorithms build a mathematical *model* based on sample data, known as "training data", in order to make predictions or decisions _without being explicitly programmed_ to perform the task.
- Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.

## Linear regression

### Linear least squares

Minimize squared difference, summed over training samples

### Minimizing with gradients

- Derivative
    - Direct solve method
    - Descent method: take small steps in diretcion of $-\frac{d y}{dx}$
- Gradient
    - Direct solve method
    - Descent method: take small steps in direction of $- \nabla f$

### Stochastic Gradient

- Basic gradient descent can ‘learn’ both $a$ and $b$, but can require many steps.
- Sub-samples of data give “good enough” gradients faster

### Linear basis function models

- Model
    - Input: original features
    - Basic function: features (transformed)
    - Parameters: weights
    - Output: prediction
- Training
    - Training set:
    - Training loss:
- Loss
- Gradient of loss
- Example


### Symptoms of under-fitting and over-fitting

- RMS: root mean squared error
- Symptoms
    - Under fitting: High training error and test error
    - Over fitting: Low training error but high test error
- Solution
    - More data: reduce the over-fitting problem
    - Regularization

### Regularized least squares

- Under-fit: High training error and test error
- Over fitting: Low training error but high test error

### Probabilistic perspective

- Maximum likelihood estimation (MLE)

- Negative log-likelihood (NLL)

- Maximum a posteriori (MAP)

---

## Logistic regression

- Often used for binary classification, but still regression
- Based on logistic sigmoid function

### Binary classification

### Probabilistic perspective

### LR vs LS

- Least squares is highly sensitive to outliers.

### Feature normalization

- Problem
- Idea: Preprocess the features so that they have comparable scale and mean value across training set.
- Normalization: Shift and scale each feature so that its mean is zero and its variance is one.

---

## Clustering

- Input: an “unlabeled” data set
- Output: group the data into “clusters” based on some measure of distance

### $K-$means clustering

- Goal: Partition $D=\\\{X_1, X_2, \cdots, X_N\\\}$ into $K$ groups.
- Idea: Introduce `centroids` $\\\{\mu_1, \mu_2, \cdots, \mu_K \\\}$, then adjust both the centroids and the cluster assignments until $X_i$ is close to its assigned centroid.
- Solution: coordinate descent

### Coordinate descent

- Problem: Can get stuck in a local minimum. No approximation guarantee.
- Choice of initial centroids has big impact on chance of sucess (good clustering)
    - Random: Set initial $\mu_k$ to be a random data point selected from $D$
    - $K-$means++: Set initial $\mu_k$ to be far away from $\\\{\mu_1, \mu_2, \cdots, \mu_K \\\}$ with high probability (spread)
        - Default for scikit-learn'n KMeans class

### $K-$means limitations

- Must guess $K$ up front, or try many $K$ and select one via a model selection criterion.
    - Elbow
- Assumes clusters of equal size; biased against finding mix of small and large clusters.
    - weighted $K-$means
- Assumes clusters are isotropic; does not model covariance or elongated structure.
    - elliptical $K-$means
- Assumes ${\|\| x-y \|\|}^2$ is a useful distance measure.
    - kernal $K-$means

### Integer $K-$means formulation

### DBSCAN

- Popular alternative to $K-$means
- Specify distance threshold and sample threshold rather than specify $K$, which may (or may not) be more intuitive

### Gaussian Mixture Models (GMM)

---

## Kernel Density Estimation

- Goal: Given samples $D=\\\{X_1, X_2, \cdots, X_N\\\}$ we want to estimate $p(X)$ at a new point $X$.
- Idea: Estimate $p(X)$ by "smoothing" the data $D$ itself.

### Kernel density estimator


### Linear Discriminant Functions

- Maximum margin principle


---

## References
* [Machine learning](https://en.wikipedia.org/wiki/Machine_learning)
